{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.jams**\n",
    "\n",
    "[Guitarist]\\_[Genre][ChordProgression]-[bpm]-[key]\\_[Solo or Chords]\n",
    "\n",
    "Example:\n",
    "\n",
    "00_BN1-129-Eb_solo.jams\n",
    "\n",
    "**.wav**\n",
    "\n",
    "[Guitarist]\\_[Genre][ChordProgression]-[bpm]-[key]\\_[Solo or Chords]\\_[Pickup]\\_[Processing]\n",
    "\n",
    "Example:\n",
    "\n",
    "00_BN1-129-Eb_solo_hex_cln.wav"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio**\n",
    "\n",
    "1. Load .wav file\n",
    "1. Resample .wav file from 44kHz to 22 kHz\n",
    "1. Constant-Q Transformation (binning!)\n",
    "1. Generate Frames (9 bins)\n",
    "\n",
    "**Labels**\n",
    "\n",
    "1. Load corresponding .jams file\n",
    "1. Extract Midi notes and timestamps\n",
    "1. Generate Labels per Frame\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import jams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../data/output/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .wav file and .jams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example files\n",
    "annot_file = \"00_BN1-129-Eb_solo.jams\"\n",
    "audio_file = \"00_BN1-129-Eb_solo_mic.wav\"\n",
    "\n",
    "# get current working directory\n",
    "swizzle_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "annot_dir = swizzle_dir + '/data/raw/annotation/'\n",
    "audio_dir = swizzle_dir + '/data/raw/audio_mono-mic/'\n",
    "\n",
    "# load annotation file and audio file\n",
    "annot = jams.load(annot_dir+annot_file)\n",
    "audio, sr = librosa.load(audio_dir+audio_file, sr=22050)\n",
    "\n",
    "# normalize audio\n",
    "audio = librosa.util.normalize(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_dir+audio_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract midinotes and timestamps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_midi = pd.DataFrame()\n",
    "\n",
    "# get midi notes for all strings for complete song\n",
    "for idx, i in enumerate(annot['annotations']['note_midi']):\n",
    "    # extract string played\n",
    "    string = [idx] * len(i['data'])\n",
    "\n",
    "    # build temporary table with midi data and string number\n",
    "    temp = pd.concat([pd.DataFrame(string), pd.DataFrame(i['data'])], axis=1)\n",
    "\n",
    "    # update df_midi\n",
    "    df_midi = pd.concat([df_midi, temp], axis=0)\n",
    "    del temp, string\n",
    "\n",
    "# calculate the end_time of a note by adding time and duration\n",
    "df_midi['end_time'] = df_midi['time'] + df_midi['duration']\n",
    "\n",
    "# correct midi notes\n",
    "df_midi['corrected_value'] = np.round(df_midi['value'], 0)\n",
    "df_midi['corrected_value'] = df_midi['corrected_value'].astype('int')\n",
    "\n",
    "\n",
    "# sort dataframe by time and reset the index\n",
    "df_midi = df_midi.sort_values(by='time').reset_index()\n",
    "\n",
    "# drop index and confidence columns\n",
    "df_midi.drop(['confidence', 'index'], axis=1, inplace=True)\n",
    "\n",
    "df_midi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in conversion table (midi <-> note <-> frequency)\n",
    "# source: https://musicinformationretrieval.com/midi_conversion_table.html\n",
    "df_conv = pd.read_csv('../data/raw/midi_annotations/conversion_table.csv', usecols=['note', 'midi-ET', 'Hertz-ET'])\n",
    "df_conv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_midi with conversion table\n",
    "df_midi = df_midi.merge(df_conv, left_on='corrected_value', right_on='midi-ET', how='left')\n",
    "\n",
    "# drop duplicates (Eb == D#, C# == Db, etc pp)\n",
    "df_midi.drop_duplicates(subset=['time', 'duration', 'value', 'end_time'], keep='last', inplace=True)\n",
    "\n",
    "# rename string column from 0 to 'string'\n",
    "df_midi = df_midi.rename(mapper={0: 'string'}, axis=1)\n",
    "\n",
    "df_midi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# scatter plot: string vs time\n",
    "fig = sns.scatterplot(data=df_midi, x='time', y='string', marker='');\n",
    "\n",
    "# annotations: notes played\n",
    "for i in df_midi.values:\n",
    "    fig.annotate(i[6], xy=(i[1], i[0]))\n",
    "\n",
    "fig.set_ylim(0, 5);\n",
    "fig.set_yticklabels(['E', 'A', 'D', 'G', 'H', 'e']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate spectrogram from .wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConstantQ transformation\n",
    "# it's a function of amplitude vs log(freq)\n",
    "# hop_length of 512 corresponds to a framerate of 43 fps\n",
    "# with for example 22 seconds, this will result in ~ 950 values\n",
    "deepest_note = 'E2'\n",
    "hop_length = 512\n",
    "\n",
    "audio_cqt = np.abs(librosa.cqt(audio, sr=sr, hop_length=hop_length, n_bins=192, bins_per_octave=24))\n",
    "\n",
    "# Convert amplitude to sound pressure level in decibel (dB)\n",
    "audio_cqt_dB = librosa.amplitude_to_db(audio_cqt, ref=np.max)\n",
    "\n",
    "# Plot the resulting spectrogram (Frequency vs. Time, colorcode: dB)\n",
    "# using specshow with y_axis='log', signals happening in the midrange are better visible\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "img = librosa.display.specshow(audio_cqt_dB, sr=sr, x_axis='time', y_axis='hz', ax=ax) # change y_axis to 'cqt_note' if you want to see the notes\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sliding windows from spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_frame(input, width: int = 9, left: bool = True):\n",
    "    \"\"\"Padding function to account for windows which left or right bounds are\n",
    "    < 0 or > len(input)\n",
    "\n",
    "    Args:\n",
    "        input (list): frame to be padded\n",
    "        width (int, optional): Window width. Defaults to 9.\n",
    "        left (bool, optional): Left or right padding. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: the padded input.\n",
    "    \"\"\"\n",
    "    orig_width = len(input)\n",
    "    padding = width - orig_width\n",
    "\n",
    "    input = list(input)\n",
    "    \n",
    "    if padding == 0:\n",
    "        return np.array(input)\n",
    "        \n",
    "    if left:\n",
    "        input = [0] * padding + input \n",
    "\n",
    "    else:\n",
    "        input = input + [0] * padding\n",
    "\n",
    "    return np.array(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(input, width: int = 9):\n",
    "    \"\"\"Sliding window function to extract windows with set width from an input array\n",
    "\n",
    "    Args:\n",
    "        input (list): Spectrogram\n",
    "        width (int, optional): Window width. Defaults to 9 frames.\n",
    "\n",
    "    Returns:\n",
    "        list: list of windows\n",
    "    \"\"\"\n",
    "    \n",
    "    frames = []\n",
    "    half_width = width//2\n",
    "\n",
    "    # i: different frequency bins\n",
    "    # j: different timepoints\n",
    "\n",
    "    for i in input:\n",
    "    \n",
    "        freq_bin = []\n",
    "    \n",
    "        for j, _ in enumerate(i):\n",
    "            \n",
    "            # set left and right bounds, so that item j is centered\n",
    "            lbound = j-half_width\n",
    "            rbound = j+half_width+1\n",
    "\n",
    "            # if bounds within input indices, just append \n",
    "            if lbound >= 0 and rbound <= len(input[0]):\n",
    "                freq_bin.append(i[lbound:rbound])\n",
    "            \n",
    "            # if left bound below zero, pad left\n",
    "            elif lbound < 0:\n",
    "                freq_bin.append(pad_frame(i[0:rbound], width, True))\n",
    "\n",
    "            # if right bound greater than input length, pad right\n",
    "            elif rbound > len(input[0]):\n",
    "                freq_bin.append(pad_frame(i[lbound:], width, False))\n",
    "                \n",
    "        frames.append(freq_bin)\n",
    "\n",
    "    return frames\n",
    "\n",
    "frames = extract_frames(audio_cqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(input):\n",
    "    \"\"\"wraps the output from extract_frames in one array\n",
    "\n",
    "    Args:\n",
    "        input (list): list of windows\n",
    "    \"\"\"\n",
    "    images = []\n",
    "\n",
    "    for j in range(len(input[0])):\n",
    "        temp = []\n",
    "        for i in input:\n",
    "            temp.append(i[j])\n",
    "\n",
    "        images.append(temp)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_windows(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_windows does this:\n",
    "# np.swapaxes(nn_input,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(get_windows(frames)[0], cmap='gray');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corresponding label data from .jams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l채nge des audiofiles in sekunden * 1/43 = endzeit von frame 1\n",
    "# f체r fenster mit frame-width = 9 --> 0 bis l채nge*9/43 = left und right timestamp\n",
    "# filter jams midi notes f체r den timeframe --> extract notes --> das sind die labels!\n",
    "# geh durch alle frames durch mit framewidth = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = librosa.get_duration(y=audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = sr // hop_length\n",
    "n_frames = fps * dur\n",
    "n_frames_int = int(np.round(n_frames, 0))\n",
    "\n",
    "# ToDo: label variables with f string\n",
    "print(dur, fps, n_frames, n_frames_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = len(get_windows(frames))\n",
    "window_labels = []\n",
    "times = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    lbound = ((i-(9//2))/n_windows) * dur\n",
    "    rbound = ((i+(9//2))/n_windows) * dur\n",
    "    window_labels.append(df_midi[(df_midi['time'] >= lbound) & (df_midi['time'] <= rbound)][['string', 'corrected_value']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique notes played for this song\n",
    "df_midi['corrected_value'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does the same as 3 cells above:\n",
    "# frame_indices = range(len(nn_input[0][0]))\n",
    "# times = librosa.frames_to_time(frame_indices, sr = sr, hop_length=hop_length)\n",
    "# times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fret_map(string_tuning: list = ['E2', 'A2', 'D3', 'G3', 'B3', 'E4']):\n",
    "\n",
    "    fret_map = []\n",
    "    \n",
    "    # get conversion table\n",
    "    # df_conv = pd.read_csv('../data/raw/midi_annotations/conversion_table.csv', usecols=['note', 'midi-ET', 'Hertz-ET'])\n",
    "\n",
    "    for n in string_tuning:\n",
    "        empty = df_conv[df_conv['note'] == n]['midi-ET']\n",
    "        temp = []\n",
    "        for f in range(20):\n",
    "            # finde index der leeren note\n",
    "            # speichere noten mit index leer+f\n",
    "            temp.append(df_conv['midi-ET'][df_conv['note'] == n].values[0] + f)\n",
    "        \n",
    "        fret_map.append(temp)\n",
    "\n",
    "\n",
    "    return fret_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_fret(window_labels):\n",
    "    tuning = [40, 45, 50, 55, 59, 64]\n",
    "    fretboard = np.zeros((len(window_labels), 6, 21))\n",
    "    \n",
    "    # 1: get string played\n",
    "    # 2: for each note played during window,\n",
    "    #    get empty string midi value (esmv)\n",
    "    # 3: subtract esmv from played note midi value\n",
    "    # 3.1: if fret played is 0,\n",
    "    # 4: replace respective value in fretboard\n",
    "    # 5: set first value to 1, if all other values are 0\n",
    "\n",
    "    for widx, window in enumerate(window_labels):\n",
    "        if window.size > 0:\n",
    "            for item in window:\n",
    "                # empty string midi value from string played (0-5)\n",
    "                esmv = tuning[item[0]]\n",
    "                # convert played note to fret\n",
    "                fret = item[1] - esmv + 1\n",
    "                # set fret to 1 in fretboard\n",
    "                fretboard[widx][item[0]][fret] = 1\n",
    "    \n",
    "        # if no note was played in window, set first values to 1\n",
    "        elif window.size == 0: \n",
    "            for idx in range(len(fretboard[widx])):\n",
    "                fretboard[widx][idx][0] = 1 \n",
    "\n",
    "    for widx in range(len(fretboard)):\n",
    "        for idx, string in enumerate(fretboard[widx]):\n",
    "            if sum(string) == 0:\n",
    "                fretboard[widx][idx][0] = 1\n",
    "        \n",
    "    # return fretboard\n",
    "    return fretboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = midi_to_fret(window_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and shape of CNN training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(data: np.array, labels: np.array):\n",
    "    # filename has no extenstion\n",
    "    filename = audio_file.split('.')[0]\n",
    "    #num_frames = self.load_rep_and_labels_from_raw_file(filename)\n",
    "    #print \"done: \" + filename + \", \" + str(num_frames) + \" frames\" \n",
    "    save_path = OUTPUT_PATH\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    np.savez(save_path + filename + \"_data_notebook.npz\", data)\n",
    "    np.savez(save_path + filename + \"_labels_notebook.npz\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loaded = np.load(OUTPUT_PATH+'/'+audio_file.split('.')[0]+'_data_notebook.npz')\n",
    "y_loaded = np.load(OUTPUT_PATH+'/'+audio_file.split('.')[0]+'_labels_notebook.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loaded['arr_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded['arr_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_labels[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test padding function\n",
    "test = [1, 1, 1, 1, 1, 1]\n",
    "assert sum(pad_frame(test, 9, True)) == sum([0, 0, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded_class = np.load(OUTPUT_PATH+'/'+audio_file.split('.')[0]+'_labels.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded_class['arr_0'][0] == y_loaded['arr_0'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ce22ec9b47a0c8818982fc2e83d14df8db5ffc759b38b6aa7930ef673c52175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
